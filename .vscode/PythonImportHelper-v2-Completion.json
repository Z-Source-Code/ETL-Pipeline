[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "create_client",
        "importPath": "supabase",
        "description": "supabase",
        "isExtraImport": true,
        "detail": "supabase",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "supabase",
        "description": "supabase",
        "isExtraImport": true,
        "detail": "supabase",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "pos_tag",
        "importPath": "nltk",
        "description": "nltk",
        "isExtraImport": true,
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "wordnet",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "create_sb_client",
        "importPath": "database",
        "description": "database",
        "isExtraImport": true,
        "detail": "database",
        "documentation": {}
    },
    {
        "label": "create_sb_client",
        "kind": 2,
        "importPath": "src.database",
        "description": "src.database",
        "peekOfCode": "def create_sb_client():\n  url: str = os.environ.get(\"SUPABASE_PROJECT_URL\")\n  key: str = os.environ.get(\"SUPABASE_API_KEY\")\n  supabase: Client = create_client(url, key)\n  return supabase",
        "detail": "src.database",
        "documentation": {}
    },
    {
        "label": "extract_data_from_supabase",
        "kind": 2,
        "importPath": "src.etl",
        "description": "src.etl",
        "peekOfCode": "def extract_data_from_supabase(table_name: str, column_name: str) -> list[str]:\n  try:\n      supabase = create_sb_client()\n      start = 0\n      limit = 1000\n      all_items = []\n      print('table_name', table_name)\n      print('column_name', column_name)\n      while True:\n          response = supabase.table(table_name).select('id', column_name).range(start, start + limit).execute()",
        "detail": "src.etl",
        "documentation": {}
    },
    {
        "label": "nltk_to_wordnet_pos",
        "kind": 2,
        "importPath": "src.etl",
        "description": "src.etl",
        "peekOfCode": "def nltk_to_wordnet_pos(nltk_tag):\n    \"\"\"\n    Descr: Convert NLTK POS tag to WordNet POS tag.\n    Input: nltk_tag\n    Output: wordnet_pos\n    \"\"\"\n    if nltk_tag.startswith('J'):\n        return wordnet.ADJ\n    elif nltk_tag.startswith('V'):\n        return wordnet.VERB",
        "detail": "src.etl",
        "documentation": {}
    },
    {
        "label": "clean_and_lemmatize",
        "kind": 2,
        "importPath": "src.etl",
        "description": "src.etl",
        "peekOfCode": "def clean_and_lemmatize(data, column_name, cleaned_column):\n  \"\"\"\n  Descr: Clean text data by removing punctuation, stopwords, and converting to lowercase.\n          Additionally, perform lemmatization on the cleaned text.\n  Input: data (text)\n  Output: cleaned and lemmatized text\n  \"\"\"\n  stop_words = set(stopwords.words('english'))\n  lemmatizer = WordNetLemmatizer() \n  clean_and_lemmatized_data  = []",
        "detail": "src.etl",
        "documentation": {}
    },
    {
        "label": "load_cleaned_data_to_supabase",
        "kind": 2,
        "importPath": "src.etl",
        "description": "src.etl",
        "peekOfCode": "def load_cleaned_data_to_supabase(table_name: str, column_name: str, cleaned_data: list[dict]):\n  try:\n    supabase = create_sb_client()\n    for item in cleaned_data:\n      identifier = item['id']\n      cleaned_col_data = item[column_name]\n      data, count = supabase.table(table_name).update({column_name: cleaned_col_data}).eq(\"id\", identifier).execute()\n    print(\"Cleaned job descriptions loaded to Supabase successfully.\")\n  except Exception as e:\n      print(\"An error occurred:\", e)",
        "detail": "src.etl",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "src.etl",
        "description": "src.etl",
        "peekOfCode": "def main():\n  nltk.download('punkt')\n  nltk.download('stopwords')\n  nltk.download('wordnet')\n  table = 'User'\n  column = 'resume'\n  cleaned_column = 'cleaned_resume'\n  data = extract_data_from_supabase(table, column)\n  print('data', data)\n  clean_and_lemmatized_data = clean_and_lemmatize(data, column, cleaned_column)",
        "detail": "src.etl",
        "documentation": {}
    }
]